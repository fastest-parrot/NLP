{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/skennedy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/skennedy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown as md\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "import regex as re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def scale(array, scale_type='standard'):\n",
    "    if scale_type == 'standard': #mean zero, std = 1\n",
    "        return (array - np.mean(array))/np.std(array)\n",
    "    if scale_type == 'min_max':\n",
    "        return (array - np.min(array))/(np.max(array) - np.min(array))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text_sub = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    display(md(f'{text_sub[:100]}'))\n",
    "    tokens = word_tokenize(text_sub)\n",
    "    display(md(f'*Raw tokens found: {len(tokens):,.0f}*'))\n",
    "    display(md(f'*Raw Vocab Size: {len(set(tokens)):,.0f}*'))\n",
    "    nltk_text = nltk.Text(tokens)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = ' '.join(\n",
    "        [w for w in nltk_text if not w in stop_words])\n",
    "    tokens_filtered = word_tokenize(filtered_text)\n",
    "    nltk_text_filtered = nltk.Text(tokens_filtered)\n",
    "    return nltk_text_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "from nltk.metrics.distance import jaro_similarity, presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***computes levenshtein distance (number of subs and insertions that need to occur to transform A --> B)***\n",
    "\n",
    "    - 3 subs:\n",
    "        - e --> l\n",
    "        - a --> i\n",
    "        - n --> m\n",
    "\n",
    "- ***percent string match*** (I'm guessing this is the number of identical positions/letters as a fraction of the total lengths?). Since only one position of 4 is a match (the first) then the answer would be 25%, but if its the total number of characters then the solution is 1/8 or 12.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sean, slim = 'sean' , 'slim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance(sean, slim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaro_similarity(sean, slim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'s'}, {'a', 'e', 'n'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sean).intersection(set(slim)), set(sean).difference(set(slim)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***My grandma loves the bible so I grabbed it from her room.....***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = '''In the beginning God created the heaven and the earth. \n",
    "        And the earth was without form, and void; and darkness was upon the face of the deep.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "in the beginning god created the heaven and the earth and the earth was without form and void and da"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Raw tokens found: 27*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Raw Vocab Size: 17*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean = clean_text(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beginning',\n",
       " 'god',\n",
       " 'created',\n",
       " 'heaven',\n",
       " 'earth',\n",
       " 'earth',\n",
       " 'without',\n",
       " 'form',\n",
       " 'void',\n",
       " 'darkness',\n",
       " 'upon',\n",
       " 'face',\n",
       " 'deep']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***She still knows its the bible - this is an iconic line from a book that has been around for the better part of 2,000 years. Even if she wasn't devout, there are plenty of contextual clues that could lead someone to guess that this is a religious text (god/created/heaven), at the very least.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'the',\n",
       " 'begin',\n",
       " 'god',\n",
       " 'creat',\n",
       " 'the',\n",
       " 'heaven',\n",
       " 'and',\n",
       " 'the',\n",
       " 'earth.',\n",
       " 'and',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'wa',\n",
       " 'without',\n",
       " 'form,',\n",
       " 'and',\n",
       " 'void;',\n",
       " 'and',\n",
       " 'dark',\n",
       " 'wa',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'face',\n",
       " 'of',\n",
       " 'the',\n",
       " 'deep.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "[stemmer.stem(x) for x in book.split(' ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The PorterStemmer incorrectly truncates two of the 27 words (created and was). 92.6 % of the words remained valid after transform.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'begin',\n",
       " 'god',\n",
       " 'creat',\n",
       " 'the',\n",
       " 'heaven',\n",
       " 'and',\n",
       " 'the',\n",
       " 'earth.',\n",
       " 'and',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'was',\n",
       " 'without',\n",
       " 'form,',\n",
       " 'and',\n",
       " 'void;',\n",
       " 'and',\n",
       " 'dark',\n",
       " 'was',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'face',\n",
       " 'of',\n",
       " 'the',\n",
       " 'deep.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "[stemmer.stem(x) for x in book.split(' ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The SnowballStemmer incorrectly truncates one of the 27 words (created). 96.3 % of the words remained valid after transform.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
